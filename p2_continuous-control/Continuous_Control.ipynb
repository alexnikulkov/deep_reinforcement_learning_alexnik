{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque, OrderedDict\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def np_to_torch(a):\n",
    "    return torch.from_numpy(a).float().to(device)\n",
    "\n",
    "\n",
    "def scale_weights(model, scale):\n",
    "    for param in model.parameters():\n",
    "        param.data.copy_(param.data * scale)\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    An actor network which simultaneously prescribes an action to take as a fucntion of state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, hidden_sizes_list):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        \n",
    "        Inputs:\n",
    "            state_size (int): Dimension of each state.\n",
    "            action_size (int): Dimension of each action.\n",
    "            seed (int): Random seed.\n",
    "            hidden_sizes_list (list): The sizes of hidden layers in the network.\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        model = OrderedDict([\n",
    "            ['bn_1', nn.BatchNorm1d(state_size)],\n",
    "            ['fc_1', nn.Linear(state_size, hidden_sizes_list[0])],\n",
    "            ['relu_1', nn.ReLU()]\n",
    "        ])\n",
    "        for i in range(1, len(hidden_sizes_list)):\n",
    "            model['bn_{}'.format(i + 1)] = nn.BatchNorm1d(hidden_sizes_list[i - 1])\n",
    "            model['fc_{}'.format(i + 1)] = nn.Linear(hidden_sizes_list[i - 1], hidden_sizes_list[i])\n",
    "            model['relu_{}'.format(i + 1)] = nn.ReLU()\n",
    "        model['bn_output'] = nn.BatchNorm1d(hidden_sizes_list[-1])\n",
    "        model['fc_output'] = nn.Linear(hidden_sizes_list[-1], action_size)\n",
    "        self.model = nn.Sequential(model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\" Forward propagation through the network.\"\"\"\n",
    "        return self.tanh(self.model(state))\n",
    "#         return self.model(state)\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A critic network which predicts the action value for a state.\n",
    "    Action is inserted as an input to the last hidden layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, hidden_sizes_list):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        \n",
    "        Inputs:\n",
    "            state_size (int): Dimension of each state.\n",
    "            action_size (int): Dimension of each action.\n",
    "            seed (int): Random seed.\n",
    "            hidden_sizes_list (list): The sizes of hidden layers in the network.\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        model_up_to_last_hidden = OrderedDict([\n",
    "            ['bn_1', nn.BatchNorm1d(state_size)],\n",
    "            ['fc_1', nn.Linear(state_size, hidden_sizes_list[0])],\n",
    "            ['relu_1', nn.ReLU()]\n",
    "        ])\n",
    "        for i in range(1, len(hidden_sizes_list) - 1):\n",
    "            model_up_to_last_hidden['bn_{}'.format(i + 1)] = nn.BatchNorm1d(hidden_sizes_list[i - 1])\n",
    "            model_up_to_last_hidden['fc_{}'.format(i + 1)] = nn.Linear(hidden_sizes_list[i - 1], hidden_sizes_list[i])\n",
    "            model_up_to_last_hidden['relu_{}'.format(i + 1)] = nn.ReLU()\n",
    "        self.model_up_to_last_hidden = nn.Sequential(model_up_to_last_hidden)\n",
    "        self.last_hidden_bn = nn.BatchNorm1d(hidden_sizes_list[-2])\n",
    "        self.last_hidden_layer = nn.Linear(hidden_sizes_list[-2], hidden_sizes_list[-1])\n",
    "        self.last_bn = nn.BatchNorm1d(hidden_sizes_list[-1] + action_size)\n",
    "        self.output_layer = nn.Linear(hidden_sizes_list[-1] + action_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Forward propagation through the network.\"\"\"\n",
    "        x = self.model_up_to_last_hidden(state)\n",
    "        x = self.relu(self.last_hidden_layer(self.last_hidden_bn(x)))\n",
    "        x = torch.cat([x, action], 1)\n",
    "        x = self.last_bn(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(pred, target, weights):\n",
    "    return torch.sum(weights * (pred - target) ** 2)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, use_prioritized_replay):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Inputs:\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "            use_prioritized_replay (boolean)\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.weights = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.use_prioritized_replay = use_prioritized_replay\n",
    "    \n",
    "    def add(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Add a set of new experiences to memory.\n",
    "        \n",
    "        Inputs:\n",
    "            states: Current state\n",
    "            actions: Action taken in the current state.\n",
    "            rewards: Reward received at the current step.\n",
    "            next_states: A state into which the transition occurs.\n",
    "            dones: An indicator of the transition learding into a terminal state.\n",
    "        \"\"\"\n",
    "        if states.ndim == 1:\n",
    "            es = [self.experience(states, actions, rewards, next_states, dones)]\n",
    "        else:\n",
    "#             print('states')\n",
    "#             print(states)\n",
    "#             print('actions')\n",
    "#             print(actions)\n",
    "#             print('rewards')\n",
    "#             print(rewards)\n",
    "#             print('next_states')\n",
    "#             print(next_states)\n",
    "#             print('dones')\n",
    "#             print(dones)\n",
    "            es = [self.experience(states[i], actions[i], rewards[i], next_states[i], dones[i]) for i in range(states.shape[0])]\n",
    "        for e in es:\n",
    "            self.memory.append(e)\n",
    "            if self.use_prioritized_replay:\n",
    "                self.weights.append(10)\n",
    "            else:\n",
    "                self.weights.append(1)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "#         if self.use_prioritized_replay:\n",
    "        indices = random.choices(range(len(self.memory)), weights=self.weights, k=self.batch_size)\n",
    "        experiences = [self.memory[i] for i in indices]\n",
    "        weights = [self.weights[i] for i in indices]\n",
    "#         else:\n",
    "#             experiences = random.sample(self.memory, k=self.batch_size)\n",
    "#             indices = None\n",
    "#             weights = None\n",
    "\n",
    "        states = np_to_torch(np.vstack([e.state for e in experiences if e is not None]))\n",
    "        actions = np_to_torch(np.vstack([e.action for e in experiences if e is not None]))\n",
    "        rewards = np_to_torch(np.vstack([e.reward for e in experiences if e is not None]))\n",
    "        next_states = np_to_torch(np.vstack([e.next_state for e in experiences if e is not None]))\n",
    "        dones = np_to_torch(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8))\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones, indices, weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def update_weights(self, indices, weights):\n",
    "        assert len(indices) == len(weights)\n",
    "        for i,w in zip(indices, weights):\n",
    "            self.weights[i] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUProcess():\n",
    "    \"\"\"\n",
    "    Orstein-Uhlenbeck process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, theta, sigma, dims):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.dims = dims\n",
    "        self.values = np.zeros(dims)\n",
    "        \n",
    "    def sample(self):\n",
    "        self.values += -self.theta * self.values + self.sigma * np.random.normal(size=self.dims)\n",
    "        return self.values\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    An RL agent which can interact with an environment and learn from replayed experiences.\n",
    "    Uses an actor-critic method for continuous actions (DDPG).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_instances, state_size, action_size, seed, actor_hidden_sizes_list, critic_hidden_sizes_list,\n",
    "                 num_iters_learn=1, update_every=4, batch_size=128, gamma=0.99, buffer_size=int(1e6),\n",
    "                 update_target_network_every=1, actor_lr0=1e-4, critic_lr0=1e-3, use_prioritized_replay=True,\n",
    "                 noise_theta=0.15, noise_sigma=0.2, weight_decay=0.0, actor_reg_loss_weight=0.0):\n",
    "        \"\"\" Initialize an Agent object.\n",
    "        \n",
    "        Inputs:\n",
    "            num_instances (int): Number of actors running in parallel.\n",
    "            state_size (int): The dimensionality of the state space.\n",
    "            action_size (int): Number of possible actions an agent can take.\n",
    "            seed (int): Randomization seed.\n",
    "            actor_hidden_sizes_list (list): The sizes of hidden layers in the Q network.\n",
    "            critic_hidden_sizes_list (list): The sizes of hidden layers in the Q network.\n",
    "            num_iters_learn (int, optional): Number of iterations to take at each step towards the targets.\n",
    "            update_every (int, optional): How often the main networks are updated (default 1).\n",
    "            batch_size (int, optional): Batch size for each upate (default 64).\n",
    "            gamma (float, optional): Temporal discount coefficient (default 0.99).\n",
    "            buffer_size (int, optional): Maximum capacity of the replay buffer (default 1e6).\n",
    "            update_target_network_every (int, optional): How often to update the target network (default 1).\n",
    "            actor_lr0 (float, optional): Initial learning rate for the actor (default 1e-4).\n",
    "            critic_lr0 (float, optional): Initial learning rate for the critic (default 1e-3).\n",
    "            use_prioritized_replay (boolean, optional): Whether to use prioritized (instead of regular) experience replay (default True).\n",
    "            noise_theta (float): Parameter of Ornstein-Uhlenbeck noise process.\n",
    "            noise_sigma (float): Parameter of Ornstein-Uhlenbeck noise process.\n",
    "            weight_decay (float): Weight decay parameter for both actor and critic networks.\n",
    "            actor_reg_loss_weight (float): Weight placed on L2 norm of actions.\n",
    "        \"\"\"\n",
    "        self.num_instances = num_instances\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.seed = seed\n",
    "        self.num_iters_learn = num_iters_learn\n",
    "        self.update_every = update_every\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.actor_lr0 = actor_lr0\n",
    "        self.critic_lr0 = critic_lr0\n",
    "        self.use_prioritized_replay = use_prioritized_replay\n",
    "        self.update_target_network_every = update_target_network_every\n",
    "        self.actor_network_main = ActorNetwork(state_size, action_size, seed, actor_hidden_sizes_list).to(device)\n",
    "        self.actor_network_target = ActorNetwork(state_size, action_size, seed, actor_hidden_sizes_list).to(device)\n",
    "        scale_weights(self.actor_network_main, 0.001)\n",
    "        self.soft_update(self.actor_network_main, self.actor_network_target, tau=1.0)\n",
    "        self.actor_network_target.eval()\n",
    "        self.critic_network_main = CriticNetwork(state_size, action_size, seed, actor_hidden_sizes_list).to(device)\n",
    "        self.critic_network_target = CriticNetwork(state_size, action_size, seed, actor_hidden_sizes_list).to(device)\n",
    "        scale_weights(self.critic_network_main, 0.001)\n",
    "        self.soft_update(self.critic_network_main, self.critic_network_target, tau=1.0)\n",
    "        self.critic_network_target.eval()\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network_main.parameters(), lr=actor_lr0, weight_decay=weight_decay)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_network_main.parameters(), lr=critic_lr0, weight_decay=weight_decay)\n",
    "        self.actor_reg_loss_weight = actor_reg_loss_weight\n",
    "        self.actor_reg_loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed, use_prioritized_replay)\n",
    "        # Initialize time step (for updating every \"update_every\" steps)\n",
    "        self.t_step = 0\n",
    "        # Initialize OU noise\n",
    "        self.noise = OUProcess(theta=noise_theta, sigma=noise_sigma, dims=(num_instances, action_size))\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):       \n",
    "        \"\"\"\n",
    "        Process a vector of state changes.\n",
    "        Periodically learn (update network) if enough experiences are available in the replay buffer.\n",
    "        \n",
    "        Inputs:\n",
    "            states: Current state\n",
    "            actions: Action taken in the current state.\n",
    "            rewards: Reward received at the current step.\n",
    "            next_states: A state into which the transition occurs.\n",
    "            dones: An indicator of the transition learding into a terminal state.\n",
    "        \"\"\"\n",
    "        self.t_step += 1\n",
    "        if self.t_step % self.update_every == 0: # Learn every \"update_every\" time steps.\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(states, actions, rewards, next_states, dones)\n",
    "                \n",
    "    def act(self, states):\n",
    "        \"\"\"Returns actions for given states as per current policy.\n",
    "        \n",
    "        Inputs:\n",
    "            states (array_like): current states\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_network_main.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_network_main(states)\n",
    "        self.actor_network_main.train()\n",
    "        return np.tanh(np.arctanh(actions.numpy()) + self.noise.sample())\n",
    "                \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Inputs:\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones, indices, weights = experiences\n",
    "        probs = np.array(weights) / sum(weights)\n",
    "        is_weights = np.power(1.0 / (len(self.memory) * probs), 0.5)\n",
    "        is_weights = is_weights / max(is_weights)\n",
    "\n",
    "        # get targets for the critic network\n",
    "        with torch.no_grad():\n",
    "            actor_target_next_actions = self.actor_network_target(next_states)\n",
    "            critic_targets = (rewards + gamma *\\\n",
    "                self.critic_network_target(next_states, actor_target_next_actions) * (1 - dones)).squeeze()\n",
    "\n",
    "        \n",
    "        for _ in range(self.num_iters_learn):\n",
    "            critic_predictions = self.critic_network_main(states, actions).squeeze()\n",
    "            critic_errors = (critic_predictions - critic_targets).squeeze().squeeze()\n",
    "            critic_loss = weighted_mse_loss(critic_predictions, critic_targets, torch.from_numpy(is_weights).float())\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "            # objective function for the actor\n",
    "            actor_proposed_actions = self.actor_network_main(states)\n",
    "            actor_objective = self.critic_network_target(states, actor_proposed_actions).mean()\n",
    "            actor_loss = -actor_objective + self.actor_reg_loss_weight * self.actor_reg_loss_fn(actor_proposed_actions,\n",
    "                                                           torch.zeros_like(actor_proposed_actions))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "        if self.use_prioritized_replay:\n",
    "            self.memory.update_weights(indices, np.power(0.05 + critic_errors.abs().data.numpy(), 0.5))\n",
    "        \n",
    "        if self.t_step % self.update_target_network_every == 0:\n",
    "            self.soft_update(self.critic_network_main, self.critic_network_target, 1e-3)\n",
    "            self.soft_update(self.actor_network_main, self.actor_network_target, 1e-3)\n",
    "\n",
    "    def soft_update(self, main_model, target_model, tau=1e-3):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Inputs:\n",
    "            main_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), main_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 49\tAverage Score: 0.50, episode took 21.80 seconds"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:117: RuntimeWarning: divide by zero encountered in arctanh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.69, 100 episodes took 4577.58 seconds\n",
      "Episode 200\tAverage Score: 0.88, 100 episodes took 2629.98 seconds\n",
      "Episode 300\tAverage Score: 0.87, 100 episodes took 2363.56 seconds\n",
      "Episode 400\tAverage Score: 0.90, 100 episodes took 2481.15 seconds\n",
      "Episode 500\tAverage Score: 0.88, 100 episodes took 2554.07 seconds\n",
      "Episode 600\tAverage Score: 0.88, 100 episodes took 8241.52 seconds\n",
      "Episode 700\tAverage Score: 0.97, 100 episodes took 2465.34 seconds\n",
      "Episode 800\tAverage Score: 0.85, 100 episodes took 2331.99 seconds\n",
      "Episode 900\tAverage Score: 0.86, 100 episodes took 2345.59 seconds\n",
      "Episode 1000\tAverage Score: 0.94, 100 episodes took 2382.06 seconds\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_1.app')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "\n",
    "agent = Agent(num_instances=num_agents,\n",
    "              state_size=state_size,\n",
    "              action_size=action_size,\n",
    "              seed=0,\n",
    "              actor_hidden_sizes_list=[128, 128],\n",
    "              critic_hidden_sizes_list=[128, 128],\n",
    "              gamma=0.99,\n",
    "              num_iters_learn=3,\n",
    "              actor_lr0=3e-5,\n",
    "              critic_lr0=3e-4,\n",
    "              weight_decay=0.00,\n",
    "              actor_reg_loss_weight=0.0,\n",
    "              update_every=3,\n",
    "              noise_sigma=0.05,\n",
    "              buffer_size=int(1e5),\n",
    "              batch_size=128,\n",
    "              use_prioritized_replay=False)\n",
    "n_episodes=1000\n",
    "max_t=10000\n",
    "actor_min_lr = 3e-7\n",
    "critic_min_lr = 3e-6\n",
    "lr_decay_episode = 0.996\n",
    "scores = []\n",
    "\n",
    "# def dqn(env, agent, scores, n_episodes=1500, max_t=1000, eps_start=0.1, eps_end=0.001,\n",
    "#         eps_decay=0.993, lr_decay_episode=0.996, min_lr=1e-6):\n",
    "#     \"\"\"Deep Q-Learning.\n",
    "    \n",
    "#     Params\n",
    "#     ======\n",
    "#         n_episodes (int): maximum number of training episodes\n",
    "#         max_t (int): maximum number of timesteps per episode\n",
    "#         eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "#         eps_end (float): minimum value of epsilon\n",
    "#         eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "#         lr_decay_episode (float): Learning rate decay multiplier (per episode)\n",
    "#         min_lr (float): Minimum learning rate (capped at the bottom at this value)\n",
    "#     \"\"\"\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "hundred_episodes_start_time = time.time()\n",
    "solved = False\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    for param_group in agent.actor_optimizer.param_groups:\n",
    "        # adjust learning rate\n",
    "        param_group['lr'] = max(agent.actor_lr0 * lr_decay_episode**i_episode, actor_min_lr)\n",
    "    for param_group in agent.critic_optimizer.param_groups:\n",
    "        # adjust learning rate\n",
    "        param_group['lr'] = max(agent.critic_lr0 * lr_decay_episode**i_episode, critic_min_lr)\n",
    "    episode_start_time = time.time()\n",
    "    env_info = env.reset(train_mode=True)[env.brain_names[0]] # reset the environment\n",
    "    states = env_info.vector_observations            # get the current state\n",
    "    score = 0\n",
    "    for t in range(max_t):\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[env.brain_names[0]]        # send the actions to the environment\n",
    "        next_states = env_info.vector_observations   # get the next states\n",
    "        rewards = env_info.rewards                   # get the reward\n",
    "        dones = env_info.local_done                  # see if episode has finished\n",
    "        score += np.mean(rewards)                                # update the score\n",
    "        agent.step(states, actions, rewards, next_states, dones)\n",
    "        states = next_states                             # roll over the state to next time step\n",
    "        if np.any(dones):                                       # exit loop if episode finished\n",
    "            break\n",
    "#     print('actions')\n",
    "#     print(actions)\n",
    "#     print('-' * 70)\n",
    "    scores_window.append(score)       # save most recent score\n",
    "    scores.append(score)              # save most recent score\n",
    "    episode_end_time = time.time()\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}, episode took {:.2f} seconds'.format(i_episode, np.mean(scores_window),\n",
    "          episode_end_time - episode_start_time), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        hundred_episodes_end_time = time.time()\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}, 100 episodes took {:.2f} seconds'.format(i_episode,\\\n",
    "            np.mean(scores_window), hundred_episodes_end_time - hundred_episodes_start_time))\n",
    "        torch.save(agent.actor_network_main.state_dict(), 'checkpoints/checkpoint_actor_{}.pth'.format(i_episode))\n",
    "        torch.save(agent.critic_network_main.state_dict(), 'checkpoints/checkpoint_critic_{}.pth'.format(i_episode))\n",
    "        hundred_episodes_start_time = time.time()\n",
    "    if (np.mean(scores_window) >= 30.0) & ~solved:\n",
    "        solved = True\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100,\\\n",
    "            np.mean(scores_window)))\n",
    "        torch.save(agent.actor_network_main.state_dict(), 'checkpoints/final_checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_network_main.state_dict(), 'checkpoints/final_checkpoint_critic.pth')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c6a6ca0371c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# plot the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(scores)), signal.savgol_filter(scores, 53, 3))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
