{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from dqn_agent import Agent\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple, deque\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.67, 100 episodes took 3488.01 seconds\n",
      "Episode 200\tAverage Score: 4.30, 100 episodes took 3109.25 seconds\n",
      "Episode 300\tAverage Score: 8.10, 100 episodes took 3129.76 seconds\n",
      "Episode 400\tAverage Score: 12.37, 100 episodes took 3131.39 seconds\n",
      "Episode 500\tAverage Score: 12.44, 100 episodes took 3204.48 seconds\n",
      "Episode 567\tAverage Score: 13.03, episode took 32.81 secondsss\n",
      "Environment solved in 467 episodes!\tAverage Score: 13.03\n",
      "Episode 600\tAverage Score: 13.16, 100 episodes took 7031.28 seconds\n",
      "Episode 654\tAverage Score: 14.00, episode took 32.59 seconds"
     ]
    }
   ],
   "source": [
    "def get_state(state_now, prev_states, prev_actions):\n",
    "    return np.concatenate((state_now,) + tuple(prev_states) + (np.array(prev_actions),))\n",
    "\n",
    "def dqn(env, agent, scores, n_episodes=1500, max_t=1000, eps_start=0.1, eps_end=0.001,\n",
    "        eps_decay=0.993, lr_decay_episode=0.996, min_lr=1e-6):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        lr_decay_episode (float): Learning rate decay multiplier (per episode)\n",
    "        min_lr (float): Minimum learning rate (capped at the bottom at this value)\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    hundred_episodes_start_time = time.time()\n",
    "    solved = False\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        for param_group in agent.optimizer_main.param_groups:\n",
    "            # adjust learning rate\n",
    "            param_group['lr'] = max(agent.lr0 * lr_decay_episode**i_episode, min_lr)\n",
    "        episode_start_time = time.time()\n",
    "        env_info = env.reset(train_mode=False)[env.brain_names[0]] # reset the environment\n",
    "        state_now = env_info.vector_observations[0]            # get the current state\n",
    "        prev_states = deque(maxlen=agent.num_frames)\n",
    "        prev_actions = deque(maxlen=agent.num_frames)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            if len(prev_states) == agent.num_frames:\n",
    "                state = get_state(state_now, prev_states, prev_actions)\n",
    "                action = agent.act(state, eps)\n",
    "                have_enough_frames = True\n",
    "            else:\n",
    "                state = None\n",
    "                # take random actions until have enough frames\n",
    "                action = np.random.randint(agent.action_size)\n",
    "                have_enough_frames = False\n",
    "            env_info = env.step(action)[env.brain_names[0]]        # send the action to the environment\n",
    "            next_state_now = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            score += reward                                # update the score\n",
    "            prev_states.append(state_now)\n",
    "            prev_actions.append(action)\n",
    "            if have_enough_frames:\n",
    "                next_state = get_state(next_state_now, prev_states, prev_actions) # note that prev_states and prev_actions are updated at this point to include the current state and action\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "            state_now = next_state_now                             # roll over the state to next time step\n",
    "            if done:                                       # exit loop if episode finished\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        episode_end_time = time.time()\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}, episode took {:.2f} seconds'.format(i_episode, np.mean(scores_window),\n",
    "              episode_end_time - episode_start_time), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            hundred_episodes_end_time = time.time()\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}, 100 episodes took {:.2f} seconds'.format(i_episode,\\\n",
    "                np.mean(scores_window), hundred_episodes_end_time - hundred_episodes_start_time))\n",
    "            torch.save(agent.QNetwork_main.state_dict(), 'checkpoints/checkpoint_{}.pth'.format(i_episode))\n",
    "            hundred_episodes_start_time = time.time()\n",
    "        if (np.mean(scores_window) >= 13.0) & ~solved:\n",
    "            solved = True\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100,\\\n",
    "                np.mean(scores_window)))\n",
    "            torch.save(agent.QNetwork_main.state_dict(), 'checkpoints/final_checkpoint.pth')\n",
    "#             break\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Banana.app\", no_graphics=False)\n",
    "env_info = env.reset(train_mode=False)[env.brain_names[0]]\n",
    "brain = env.brains[env.brain_names[0]]\n",
    "\n",
    "agent = Agent(state_size=len(env_info.vector_observations[0]),\n",
    "              action_size=brain.vector_action_space_size,\n",
    "              seed=0,\n",
    "              hidden_sizes_list=[32, 32],\n",
    "              num_frames=0,\n",
    "              lr0=5e-4,\n",
    "              gamma=0.99,\n",
    "              num_iters_learn=3)\n",
    "scores = []\n",
    "dqn(env, agent, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "plt.plot(np.arange(len(scores)), signal.savgol_filter(scores, 53, 3))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_index = None\n",
    "# best_score = 0\n",
    "# for i in range(100, (len(scores) + 1), 100):\n",
    "#     score = np.mean(scores[i-100:i])\n",
    "#     if score > best_score:\n",
    "#         best_score = score\n",
    "#         best_model_index = i\n",
    "# print(best_model_index, best_score)\n",
    "# checkpoint = torch.load('checkpoints/checkpoint_{}.pth'.format(best_model_index))\n",
    "checkpoint = torch.load('checkpoints/final_checkpoint.pth')\n",
    "agent.QNetwork_main.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_index = 1300\n",
    "# checkpoint = torch.load('checkpoint_{}.pth'.format(best_model_index))\n",
    "# agent.QNetwork_main.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "env_info = env.reset(train_mode=False)[env.brain_names[0]] # reset the environment\n",
    "state_now = env_info.vector_observations[0]            # get the current state\n",
    "prev_states = deque(maxlen=agent.num_frames)\n",
    "prev_actions = deque(maxlen=agent.num_frames)\n",
    "score = 0\n",
    "while True:\n",
    "    if len(prev_states) == agent.num_frames:\n",
    "        state = get_state(state_now, prev_states, prev_actions)\n",
    "        action = agent.act(state, eps=0.0)\n",
    "        have_enough_frames = True\n",
    "    else:\n",
    "        state = None\n",
    "        # take random actions until have enough frames\n",
    "        action = np.random.randint(agent.action_size)\n",
    "        have_enough_frames = False\n",
    "    env_info = env.step(action)[env.brain_names[0]]        # send the action to the environment\n",
    "    next_state_now = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    prev_states.append(state_now)\n",
    "    prev_actions.append(action)\n",
    "    if have_enough_frames:\n",
    "        next_state = get_state(next_state_now, prev_states, prev_actions) # note that prev_states and prev_actions are updated at this point to include the current state and action\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "    state_now = next_state_now                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
