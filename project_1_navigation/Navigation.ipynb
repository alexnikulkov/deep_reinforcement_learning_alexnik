{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from dqn_agent import Agent\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityTimeOutException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0f6de520e7e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Banana.app\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0menv_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mbrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrains\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, worker_id, base_port, curriculum, seed, docker_training, no_graphics)\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0maca_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_academy_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_init_parameters_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnityTimeOutException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/unityagents/environment.py\u001b[0m in \u001b[0;36msend_academy_parameters\u001b[0;34m(self, init_parameters)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_initialization_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_initialization_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap_unity_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnityRLInput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnityOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/unityagents/rpc_communicator.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             raise UnityTimeOutException(\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0;34m\"The Unity environment took too long to respond. Make sure that :\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0;34m\"\\t The environment does not need user interaction to launch\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;34m\"\\t The Academy and the External Brain(s) are attached to objects in the Scene\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnityTimeOutException\u001b[0m: The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Academy and the External Brain(s) are attached to objects in the Scene\n\t The environment and the Python interface have compatible versions."
     ]
    }
   ],
   "source": [
    "def get_state(state_now, prev_states, prev_actions):\n",
    "    return np.concatenate((state_now,) + tuple(prev_states) + (np.array(prev_actions),))\n",
    "\n",
    "def dqn(env, agent, scores, n_episodes=1500, max_t=1000, eps_start=1.0, eps_end=0.01,\n",
    "        eps_decay=0.997, lr_decay_episode=0.997, min_lr=1e-5):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        lr_decay_episode (float): Learning rate decay multiplier (per episode)\n",
    "        min_lr (float): Minimum learning rate (capped at the bottom at this value)\n",
    "    \"\"\"\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    hundred_episodes_start_time = time.time()\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        for param_group in agent.optimizer_main.param_groups:\n",
    "            # adjust learning rate\n",
    "            param_group['lr'] = max(agent.lr0 * lr_decay_episode**i_episode, min_lr)\n",
    "        episode_start_time = time.time()\n",
    "        env_info = env.reset(train_mode=False)[env.brain_names[0]] # reset the environment\n",
    "        state_now = env_info.vector_observations[0]            # get the current state\n",
    "        prev_states = deque(maxlen=agent.num_frames)\n",
    "        prev_actions = deque(maxlen=agent.num_frames)\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            if len(prev_states) == agent.num_frames:\n",
    "                state = get_state(state_now, prev_states, prev_actions)\n",
    "                action = agent.act(state, eps)\n",
    "                have_enough_frames = True\n",
    "            else:\n",
    "                state = None\n",
    "                # take random actions until have enough frames\n",
    "                action = np.random.randint(agent.action_size)\n",
    "                have_enough_frames = False\n",
    "            env_info = env.step(action)[env.brain_names[0]]        # send the action to the environment\n",
    "            next_state_now = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            score += reward                                # update the score\n",
    "            prev_states.append(state_now)\n",
    "            prev_actions.append(action)\n",
    "            if have_enough_frames:\n",
    "                next_state = get_state(next_state_now, prev_states, prev_actions) # note that prev_states and prev_actions are updated at this point to include the current state and action\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "            state_now = next_state_now                             # roll over the state to next time step\n",
    "            if done:                                       # exit loop if episode finished\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        episode_end_time = time.time()\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}, episode took {:.2f} seconds'.format(i_episode, np.mean(scores_window),\n",
    "              episode_end_time - episode_start_time), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            hundred_episodes_end_time = time.time()\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}, 100 episodes took {:.2f} seconds'.format(i_episode,\\\n",
    "                np.mean(scores_window), hundred_episodes_end_time - hundred_episodes_start_time))\n",
    "            torch.save(agent.QNetwork_main.state_dict(), 'checkpoints/checkpoint_{}.pth'.format(i_episode))\n",
    "            hundred_episodes_start_time = time.time()\n",
    "        if np.mean(scores_window) >= 13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100,\\\n",
    "                np.mean(scores_window)))\n",
    "            torch.save(agent.QNetwork_main.state_dict(), 'checkpoints/final_checkpoint.pth')\n",
    "            break\n",
    "\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "env_info = env.reset(train_mode=False)[env.brain_names[0]]\n",
    "brain = env.brains[env.brain_names[0]]\n",
    "\n",
    "agent = Agent(state_size=len(env_info.vector_observations[0]),\n",
    "              action_size=brain.vector_action_space_size,\n",
    "              seed=0,\n",
    "              hidden_sizes_list=[64, 32, 16],\n",
    "              num_frames=0,\n",
    "              lr0=5e-4,\n",
    "              gamma=0.99,\n",
    "              num_iters_learn=3)\n",
    "scores = []\n",
    "dqn(env, agent, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "plt.plot(np.arange(len(scores)), signal.savgol_filter(scores, 53, 3))\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_index = None\n",
    "best_score = 0\n",
    "for i in range(100, (len(scores) + 100), 100):\n",
    "    score = np.mean(scores[i-100:i])\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_index = i\n",
    "print(best_model_index, best_score)\n",
    "checkpoint = torch.load('checkpoints/checkpoint_{}.pth'.format(best_model_index))\n",
    "agent.QNetwork_main.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_index = 1300\n",
    "# checkpoint = torch.load('checkpoint_{}.pth'.format(best_model_index))\n",
    "# agent.QNetwork_main.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "env_info = env.reset(train_mode=False)[env.brain_names[0]] # reset the environment\n",
    "state_now = env_info.vector_observations[0]            # get the current state\n",
    "prev_states = deque(maxlen=agent.num_frames)\n",
    "prev_actions = deque(maxlen=agent.num_frames)\n",
    "score = 0\n",
    "while True:\n",
    "    if len(prev_states) == agent.num_frames:\n",
    "        state = get_state(state_now, prev_states, prev_actions)\n",
    "        action = agent.act(state, eps=0.0)\n",
    "        have_enough_frames = True\n",
    "    else:\n",
    "        state = None\n",
    "        # take random actions until have enough frames\n",
    "        action = np.random.randint(agent.action_size)\n",
    "        have_enough_frames = False\n",
    "    env_info = env.step(action)[env.brain_names[0]]        # send the action to the environment\n",
    "    next_state_now = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    prev_states.append(state_now)\n",
    "    prev_actions.append(action)\n",
    "    if have_enough_frames:\n",
    "        next_state = get_state(next_state_now, prev_states, prev_actions) # note that prev_states and prev_actions are updated at this point to include the current state and action\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "    state_now = next_state_now                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
