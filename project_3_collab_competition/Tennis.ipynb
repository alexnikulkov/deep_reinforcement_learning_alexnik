{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque, OrderedDict\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from scipy import signal\n",
    "from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mse_loss(pred, target, weights):\n",
    "    return torch.mean(weights * (pred - target) ** 2)\n",
    "\n",
    "def np_to_torch(a):\n",
    "    \"\"\"\n",
    "    Convert numpy array into a pytorch tensor and send to device\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(a).float().to(device)\n",
    "\n",
    "\n",
    "def scale_weights(model, scale):\n",
    "    \"\"\"\n",
    "    Multiply all model weights by a given multiplier.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.data.copy_(param.data * scale)\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    An actor network which simultaneously prescribes an action to take as a fucntion of state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, hidden_sizes_list):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        \n",
    "        Inputs:\n",
    "            state_size (int): Dimension of each state.\n",
    "            action_size (int): Dimension of each action.\n",
    "            seed (int): Random seed.\n",
    "            hidden_sizes_list (list): The sizes of hidden layers in the network.\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        model = OrderedDict([\n",
    "            ['fc_1', nn.Linear(state_size, hidden_sizes_list[0])],\n",
    "            ['relu_1', nn.ReLU()]\n",
    "        ])\n",
    "        for i in range(1, len(hidden_sizes_list)):\n",
    "            model['fc_{}'.format(i + 1)] = nn.Linear(hidden_sizes_list[i - 1], hidden_sizes_list[i])\n",
    "            model['relu_{}'.format(i + 1)] = nn.ReLU()\n",
    "        model['fc_output'] = nn.Linear(hidden_sizes_list[-1], action_size)\n",
    "        self.model = nn.Sequential(model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\" Forward propagation through the network.\"\"\"\n",
    "        return self.tanh(self.model(state))\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A critic network which predicts the action value for a state.\n",
    "    Action is inserted as an input to the last hidden layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, hidden_sizes_list):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        \n",
    "        Inputs:\n",
    "            state_size (int): Dimension of each state.\n",
    "            action_size (int): Dimension of each action.\n",
    "            seed (int): Random seed.\n",
    "            hidden_sizes_list (list): The sizes of hidden layers in the network.\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        model_first_part = OrderedDict([\n",
    "            ['fc_1', nn.Linear(2 * state_size, hidden_sizes_list[0])],\n",
    "            ['relu_1', nn.ReLU()]\n",
    "        ])\n",
    "        self.model_first_part = nn.Sequential(model_first_part)\n",
    "        model_second_part = OrderedDict()\n",
    "        for i in range(1, len(hidden_sizes_list)):\n",
    "            model_second_part['fc_{}'.format(i + 1)] = nn.Linear(hidden_sizes_list[i - 1] + 2 * action_size * (i==1), hidden_sizes_list[i])\n",
    "            model_second_part['relu_{}'.format(i + 1)] = nn.ReLU()\n",
    "        self.model_second_part = nn.Sequential(model_second_part)\n",
    "        self.output_layer = nn.Linear(hidden_sizes_list[-1], 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\" Forward propagation through the network.\"\"\"\n",
    "        x = self.model_first_part(state)\n",
    "        x = torch.cat([x, action], 1)\n",
    "        x = self.model_second_part(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, use_prioritized_replay):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Inputs:\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "            use_prioritized_replay (boolean)\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.weights = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.use_prioritized_replay = use_prioritized_replay\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\n",
    "        \n",
    "        Inputs:\n",
    "            state: Current state\n",
    "            action: Action taken in the current state.\n",
    "            reward: Reward received at the current step.\n",
    "            next_state: A state into which the transition occurs.\n",
    "            done: An indicator of the transition learding into a terminal state.\n",
    "        \"\"\"\n",
    "        es = [self.experience(states, actions, rewards, next_states, dones)]\n",
    "        for e in es:\n",
    "            self.memory.append(e)\n",
    "            if self.use_prioritized_replay:\n",
    "                self.weights.append(10)\n",
    "            else:\n",
    "                self.weights.append(1)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        indices = random.choices(range(len(self.memory)), weights=self.weights, k=self.batch_size)\n",
    "        experiences = [self.memory[i] for i in indices]\n",
    "        weights = [self.weights[i] for i in indices]\n",
    "        \n",
    "        states = {}\n",
    "        actions = {}\n",
    "        rewards = {}\n",
    "        next_states = {}\n",
    "        dones = {}\n",
    "        for i in range(2):\n",
    "            states[i] = torch.from_numpy(np.vstack([e.state[i,:] for e in experiences if e is not None])).float().to(device)\n",
    "            actions[i] = torch.from_numpy(np.vstack([e.action[i,:] for e in experiences if e is not None])).float().to(device)\n",
    "            rewards[i] = torch.from_numpy(np.vstack([e.reward[i] for e in experiences if e is not None])).float().to(device)\n",
    "            next_states[i] = torch.from_numpy(np.vstack([e.next_state[i,:] for e in experiences if e is not None])).float().to(device)\n",
    "            dones[i] = torch.from_numpy(np.vstack([e.done[i] for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones, indices, weights)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def update_weights(self, indices, weights):\n",
    "        assert len(indices) == len(weights)\n",
    "        for i,w in zip(indices, weights):\n",
    "            self.weights[i] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUProcess():\n",
    "    \"\"\"\n",
    "    Orstein-Uhlenbeck process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, theta, sigma, dims, mu=0.0):\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.dims = dims\n",
    "        self.mu = mu\n",
    "        self.values = np.ones(dims) * mu\n",
    "        \n",
    "    def sample(self):\n",
    "        self.values += self.theta * (self.mu - self.values) + self.sigma * np.random.normal(size=self.dims)\n",
    "        return self.values\n",
    "    \n",
    "    def reset(self):\n",
    "        self.values = np.ones(self.dims) * self.mu\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "    An RL agent which can interact with an environment and learn from replayed experiences.\n",
    "    Uses an actor-critic method for continuous actions (DDPG).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_instances, state_size, action_size, seed, actor_hidden_sizes_list, critic_hidden_sizes_list,\n",
    "                 num_iters_learn=1, update_every=4, use_prioritized_replay=True, batch_size=128, gamma=0.99, buffer_size=int(1e6),\n",
    "                 update_target_network_every=1, actor_lr0=1e-4, critic_lr0=1e-3,\n",
    "                 noise_theta=0.15, noise_sigma=0.2, noise_sigma_decay=1.0, weight_decay=0.0,\n",
    "                 actor_reg_loss_weight=0.0, random_seed=0):\n",
    "        \"\"\" Initialize an Agent object.\n",
    "        \n",
    "        Inputs:\n",
    "            num_instances (int): Number of actors running in parallel.\n",
    "            state_size (int): The dimensionality of the state space.\n",
    "            action_size (int): Number of possible actions an agent can take.\n",
    "            seed (int): Randomization seed.\n",
    "            actor_hidden_sizes_list (list): The sizes of hidden layers in the actor network.\n",
    "            critic_hidden_sizes_list (list): The sizes of hidden layers in the critic network.\n",
    "            num_iters_learn (int, optional): Number of iterations to take at each step towards the targets.\n",
    "            update_every (int, optional): How often the main networks are updated (default 1).\n",
    "            batch_size (int, optional): Batch size for each upate (default 64).\n",
    "            gamma (float, optional): Temporal discount coefficient (default 0.99).\n",
    "            buffer_size (int, optional): Maximum capacity of the replay buffer (default 1e6).\n",
    "            update_target_network_every (int, optional): How often to update the target network (default 1).\n",
    "            actor_lr0 (float, optional): Initial learning rate for the actor (default 1e-4).\n",
    "            critic_lr0 (float, optional): Initial learning rate for the critic (default 1e-3).\n",
    "            noise_theta (float): Parameter of Ornstein-Uhlenbeck noise process.\n",
    "            noise_sigma (float): Parameter of Ornstein-Uhlenbeck noise process.\n",
    "            noise_sigma_decay (float): Multiplier for noise_sigma (per episode).\n",
    "            weight_decay (float): Weight decay parameter for both actor and critic networks.\n",
    "            actor_reg_loss_weight (float): Weight placed on L2 norm of actions.\n",
    "            random_seed (int): Seed for pseudo-random numbers.\n",
    "        \"\"\"\n",
    "        self.num_instances = num_instances\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.seed = seed\n",
    "        self.num_iters_learn = num_iters_learn\n",
    "        self.update_every = update_every\n",
    "        self.use_prioritized_replay = use_prioritized_replay\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.actor_lr0 = actor_lr0\n",
    "        self.critic_lr0 = critic_lr0\n",
    "        self.update_target_network_every = update_target_network_every\n",
    "        self.actor_network_main = ActorNetwork(state_size, action_size, seed, actor_hidden_sizes_list).to(device)\n",
    "        self.actor_network_target = ActorNetwork(state_size, action_size, seed, actor_hidden_sizes_list).to(device)\n",
    "        self.soft_update(self.actor_network_main, self.actor_network_target, tau=1.0)\n",
    "        self.actor_network_target.eval()\n",
    "        \n",
    "        self.critic_network_main = CriticNetwork(state_size, action_size, seed, critic_hidden_sizes_list).to(device)\n",
    "        self.critic_network_target = CriticNetwork(state_size, action_size, seed, critic_hidden_sizes_list).to(device)\n",
    "        self.soft_update(self.critic_network_main, self.critic_network_target, tau=1.0)\n",
    "        self.critic_network_target.eval()\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor_network_main.parameters(), lr=actor_lr0, weight_decay=weight_decay)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_network_main.parameters(), lr=critic_lr0, weight_decay=weight_decay)\n",
    "        self.actor_reg_loss_weight = actor_reg_loss_weight\n",
    "        self.actor_reg_loss_fn = lambda x: F.mse_loss(x, torch.zeros_like(x))\n",
    "        \n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size, seed, use_prioritized_replay)\n",
    "        # Initialize time step (for updating every \"update_every\" steps)\n",
    "        self.t_step = 0\n",
    "        # Initialize OU noise\n",
    "        self.noise = OUProcess(theta=noise_theta, sigma=noise_sigma, dims=(num_instances, action_size))\n",
    "        self.noise_sigma_decay = noise_sigma_decay\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones, num_agent):       \n",
    "        \"\"\"\n",
    "        Process a vector of state changes.\n",
    "        Periodically learn (update network) if enough experiences are available in the replay buffer.\n",
    "        \n",
    "        Inputs:\n",
    "            states: Current state\n",
    "            actions: Action taken in the current state.\n",
    "            rewards: Reward received at the current step.\n",
    "            next_states: A state into which the transition occurs.\n",
    "            dones: An indicator of the transition learding into a terminal state.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(states, actions, rewards, next_states, dones)\n",
    "        self.t_step += 1\n",
    "        \n",
    "        if self.t_step % self.update_every == 0: # Learn every \"update_every\" time steps.\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma, num_agent)\n",
    "        \n",
    "               \n",
    "    def act(self, states):\n",
    "        \"\"\"Returns actions for given states as per current policy.\n",
    "        \n",
    "        Inputs:\n",
    "            states (array_like): current states\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_network_main.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_network_main(states).cpu().data.numpy()\n",
    "        self.actor_network_main.train()\n",
    "        return np.tanh(np.arctanh(actions) + self.noise.sample())\n",
    "#         return np.clip(actions + self.noise.sample(), -1, 1)\n",
    "\n",
    "                \n",
    "    def learn(self, experiences, gamma, num_agent):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Inputs:\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones, indices, weights = experiences\n",
    "        probs = np.array(weights) / sum(weights)\n",
    "        is_weights = np.power(1.0 / (len(self.memory) * probs), 0.5)\n",
    "        is_weights = is_weights / max(is_weights)\n",
    "        \n",
    "        for _ in range(self.num_iters_learn):\n",
    "            # get targets for the critic network\n",
    "            with torch.no_grad():\n",
    "                actor_target_next_actions = self.actor_network_target(next_states[num_agent])\n",
    "                other_actor_target_next_actions = self.other_agent.actor_network_target(next_states[1-num_agent])\n",
    "                both_agents_next_actions = torch.cat((actor_target_next_actions, other_actor_target_next_actions), 1)\n",
    "                both_agents_next_states = torch.cat((next_states[num_agent], next_states[1-num_agent]), 1)\n",
    "                Q_targets_next = self.critic_network_target(both_agents_next_states, both_agents_next_actions)\n",
    "                critic_targets = (rewards[num_agent] + gamma * Q_targets_next * (1 - dones[num_agent])).squeeze()\n",
    "            \n",
    "            # update critic\n",
    "            both_agents_states = torch.cat((states[num_agent], states[1-num_agent]), 1)\n",
    "            both_agents_actions = torch.cat((actions[num_agent], actions[1-num_agent]), 1)\n",
    "            critic_predictions = self.critic_network_main(both_agents_states, both_agents_actions).squeeze()\n",
    "            critic_errors = (critic_predictions - critic_targets).squeeze()\n",
    "            critic_loss = weighted_mse_loss(critic_predictions, critic_targets, np_to_torch(is_weights).float())\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "            # update actor\n",
    "            actor_proposed_actions = self.actor_network_main(states[num_agent])\n",
    "            with torch.no_grad():\n",
    "                other_agent_actor_proposed_actions = self.other_agent.actor_network_target(states[1-num_agent])\n",
    "            actor_objective = (self.critic_network_main(both_agents_states, torch.cat((actor_proposed_actions, other_agent_actor_proposed_actions), 1))\\\n",
    "                               * np_to_torch(is_weights)).mean()\n",
    "            actor_loss = -actor_objective + self.actor_reg_loss_weight * self.actor_reg_loss_fn(actor_proposed_actions)\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "        \n",
    "        if self.use_prioritized_replay:\n",
    "            self.memory.update_weights(indices, np.power(critic_errors.abs().data.numpy(), 1.5))\n",
    "        \n",
    "        if self.t_step % self.update_target_network_every == 0:\n",
    "            self.soft_update(self.critic_network_main, self.critic_network_target, 1e-3)\n",
    "            self.soft_update(self.actor_network_main, self.actor_network_target, 1e-3)\n",
    "\n",
    "    def soft_update(self, main_model, target_model, tau=3e-2):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Inputs:\n",
    "            main_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), main_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [[-0.10820136  0.24186577]\n",
      " [ 0.95989601  0.85204035]]\n",
      "Current noise:0.30\n",
      "Current actor LR:0.000869\n",
      "Current critic LR:0.002608\n",
      "Average score per actor:[0.0046 0.0054]\n",
      "Episode 100\tCurrent Score: 0.09\tAverage Score: 0.02, episode took 0.47 seconds\n",
      "Actions: [[ 0.0576414  -0.50149076]\n",
      " [ 0.41481171  0.02698048]]\n",
      "Current noise:0.30\n",
      "Current actor LR:0.000756\n",
      "Current critic LR:0.002267\n",
      "Average score per actor:[0.0101 0.0008]\n",
      "Episode 200\tCurrent Score: 0.00\tAverage Score: 0.02, episode took 0.23 seconds\n",
      "Actions: [[ 0.02851862 -0.85279162]\n",
      " [-0.64108417  0.26303867]]\n",
      "Current noise:0.30\n",
      "Current actor LR:0.000657\n",
      "Current critic LR:0.001971\n",
      "Average score per actor:[0.0047 0.0043]\n",
      "Episode 300\tCurrent Score: 0.10\tAverage Score: 0.02, episode took 0.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:130: RuntimeWarning: divide by zero encountered in arctanh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [[0.99904672 0.33386424]\n",
      " [0.05576124 0.52024812]]\n",
      "Current noise:0.22\n",
      "Current actor LR:0.000571\n",
      "Current critic LR:0.001713\n",
      "Average score per actor:[-0.0035  0.0135]\n",
      "Episode 400\tCurrent Score: 0.09\tAverage Score: 0.02, episode took 0.55 seconds\n",
      "Actions: [[ 0.59705752  0.62849489]\n",
      " [-0.04818483  0.2755071 ]]\n",
      "Current noise:0.16\n",
      "Current actor LR:0.000496\n",
      "Current critic LR:0.001489\n",
      "Average score per actor:[0.0091 0.0129]\n",
      "Episode 500\tCurrent Score: 0.00\tAverage Score: 0.03, episode took 0.25 seconds\n",
      "Actions: [[ 0.30117287  0.04586172]\n",
      " [ 0.31339432 -0.37591949]]\n",
      "Current noise:0.12\n",
      "Current actor LR:0.000431\n",
      "Current critic LR:0.001294\n",
      "Average score per actor:[0.0055 0.0024]\n",
      "Episode 600\tCurrent Score: 0.00\tAverage Score: 0.02, episode took 0.26 seconds\n",
      "Actions: [[-0.03146085 -0.53021133]\n",
      " [ 0.13602211  0.09467535]]\n",
      "Current noise:0.09\n",
      "Current actor LR:0.000375\n",
      "Current critic LR:0.001125\n",
      "Average score per actor:[0.0124 0.0016]\n",
      "Episode 700\tCurrent Score: 0.00\tAverage Score: 0.02, episode took 0.28 seconds\n",
      "Actions: [[-0.03768715  0.1218946 ]\n",
      " [ 0.53112317  0.09602883]]\n",
      "Current noise:0.07\n",
      "Current actor LR:0.000326\n",
      "Current critic LR:0.000978\n",
      "Average score per actor:[0.0201 0.0029]\n",
      "Episode 800\tCurrent Score: 0.00\tAverage Score: 0.03, episode took 0.32 seconds\n",
      "Actions: [[0.81713057 0.47627438]\n",
      " [0.61377559 0.05127794]]\n",
      "Current noise:0.05\n",
      "Current actor LR:0.000283\n",
      "Current critic LR:0.000850\n",
      "Average score per actor:[0.0232 0.0147]\n",
      "Episode 900\tCurrent Score: 0.00\tAverage Score: 0.04, episode took 0.30 seconds\n",
      "Actions: [[ 0.99960027 -0.95701489]\n",
      " [ 0.4762711   0.31676594]]\n",
      "Current noise:0.04\n",
      "Current actor LR:0.000246\n",
      "Current critic LR:0.000739\n",
      "Average score per actor:[0.0406 0.0442]\n",
      "Episode 1000\tCurrent Score: 0.00\tAverage Score: 0.07, episode took 0.30 seconds\n",
      "Actions: [[ 0.94691402 -0.95506241]\n",
      " [ 0.51286377  0.06498146]]\n",
      "Current noise:0.03\n",
      "Current actor LR:0.000214\n",
      "Current critic LR:0.000642\n",
      "Average score per actor:[0.0852 0.0708]\n",
      "Episode 1100\tCurrent Score: 0.10\tAverage Score: 0.12, episode took 1.20 seconds\n",
      "Actions: [[ 0.0032295  -0.1423456 ]\n",
      " [ 0.38690908 -0.00172017]]\n",
      "Current noise:0.02\n",
      "Current actor LR:0.000186\n",
      "Current critic LR:0.000558\n",
      "Average score per actor:[0.0884 0.0656]\n",
      "Episode 1200\tCurrent Score: 0.10\tAverage Score: 0.11, episode took 1.21 seconds\n",
      "Actions: [[ 0.08950948  0.246061  ]\n",
      " [-0.44903652  0.25346749]]\n",
      "Current noise:0.01\n",
      "Current actor LR:0.000162\n",
      "Current critic LR:0.000485\n",
      "Average score per actor:[0.052 0.065]\n",
      "Episode 1300\tCurrent Score: 0.09\tAverage Score: 0.09, episode took 0.79 seconds\n",
      "Actions: [[-0.91689026  0.69802751]\n",
      " [ 0.23739308  0.19144008]]\n",
      "Current noise:0.01\n",
      "Current actor LR:0.000141\n",
      "Current critic LR:0.000422\n",
      "Average score per actor:[0.1009 0.1079]\n",
      "Episode 1400\tCurrent Score: 0.10\tAverage Score: 0.14, episode took 0.89 seconds\n",
      "Actions: [[-0.92471841  0.99992066]\n",
      " [ 0.04667069  0.30295342]]\n",
      "Current noise:0.01\n",
      "Current actor LR:0.000122\n",
      "Current critic LR:0.000367\n",
      "Average score per actor:[0.0873 0.0856]\n",
      "Episode 1500\tCurrent Score: 0.20\tAverage Score: 0.11, episode took 2.15 seconds\n",
      "Actions: [[-0.54043195 -0.50956809]\n",
      " [ 0.11871749 -0.14311187]]\n",
      "Current noise:0.01\n",
      "Current actor LR:0.000106\n",
      "Current critic LR:0.000319\n",
      "Average score per actor:[0.2326 0.2113]\n",
      "Episode 1600\tCurrent Score: 0.30\tAverage Score: 0.25, episode took 5.14 seconds\n",
      "Actions: [[-0.98307633  0.87838464]\n",
      " [-0.25142446  0.14659204]]\n",
      "Current noise:0.00\n",
      "Current actor LR:0.000092\n",
      "Current critic LR:0.000277\n",
      "Average score per actor:[0.2732 0.2758]\n",
      "Episode 1700\tCurrent Score: 0.60\tAverage Score: 0.31, episode took 9.30 seconds\n",
      "\n",
      "Environment solved in 1673 episodes!\tAverage Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Tennis.app')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "num_agents = states.shape[0]\n",
    "state_size = states.shape[1]\n",
    "\n",
    "\n",
    "def learn_maddpg(env, agents, scores, n_episodes=3000, num_episodes_random_actions = 300, max_t=1000, save_every=100,\n",
    "        print_every = 100, lr_decay_episode=0.9986, actor_min_lr = 1e-7, critic_min_lr = 3e-7):\n",
    "    \"\"\"Mutli-Agent Deep Deterministic Policy Gradient\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        env: An environment\n",
    "        agents: A list of RL agents\n",
    "        scores (list): A list where the scores will be saved.\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        num_episodes_random_actions (int): How long to do pure exploration\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        save_every (int): How often to save the checkpoints\n",
    "        print_every (int): How often to print progress\n",
    "        lr_decay_episode (float): Learning rate decay multiplier (per episode)\n",
    "        actor_min_lr (float): Minimum actor learning rate (capped at the bottom at this value)\n",
    "        critic_min_lr (float): Minimum critic learning rate (capped at the bottom at this value)\n",
    "    \"\"\"\n",
    "    num_players = len(agents)\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    scores_window_2actors = deque(maxlen=100)  # last 100 scores\n",
    "    hundred_episodes_start_time = time.time()\n",
    "    solved = False\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        for agent in agents:\n",
    "            for param_group in agent.actor_optimizer.param_groups:\n",
    "                # adjust actor learning rate\n",
    "                param_group['lr'] = max(agent.actor_lr0 * lr_decay_episode**i_episode, actor_min_lr)\n",
    "                actor_lr = param_group['lr']\n",
    "            for param_group in agent.critic_optimizer.param_groups:\n",
    "                # adjust critic learning rate\n",
    "                param_group['lr'] = max(agent.critic_lr0 * lr_decay_episode**i_episode, critic_min_lr)\n",
    "                critic_lr = param_group['lr']\n",
    "            # reduce action noise variance\n",
    "            if i_episode > num_episodes_random_actions:\n",
    "                agent.noise.reset()\n",
    "                agent.noise.sigma *= agent.noise_sigma_decay\n",
    "        episode_start_time = time.time()\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        states = env_info.vector_observations            # get the current state\n",
    "        scores_this_episode = np.zeros(num_players)\n",
    "        for t in range(max_t):\n",
    "            if i_episode > num_episodes_random_actions:\n",
    "                actions = np.empty((num_players, action_size))\n",
    "                for i, agent in enumerate(agents):\n",
    "                    actions[i,:] = agent.act(states[i:(i+1),:])\n",
    "            else:\n",
    "                actions = np.random.uniform(-1, 1, size=(num_players, action_size))\n",
    "            env_info = env.step(actions)[env.brain_names[0]]        # send the actions to the environment\n",
    "            next_states = env_info.vector_observations   # get the next states\n",
    "            rewards = env_info.rewards                   # get the reward\n",
    "            dones = env_info.local_done                  # see if episode has finished\n",
    "            scores_this_episode += rewards                                # update the score\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.step(states, actions, rewards, next_states, dones, num_agent=i)\n",
    "            states = next_states                             # roll over the state to next time step\n",
    "            if np.any(dones):                                       # exit loop if episode finished\n",
    "                break\n",
    "        scores_window.append(np.max(scores_this_episode))       # save most recent score\n",
    "        scores_window_2actors.append(scores_this_episode)\n",
    "        scores.append(np.max(scores_this_episode))              # save most recent score\n",
    "        episode_end_time = time.time()\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Actions: {}'.format(actions))\n",
    "            print('Current noise:{:.2f}'.format(agents[0].noise.sigma))\n",
    "            print('Current actor LR:{:.6f}'.format(actor_lr))\n",
    "            print('Current critic LR:{:.6f}'.format(critic_lr))\n",
    "            print('Average score per actor:{}'.format(np.array(scores_window_2actors).mean(0)))\n",
    "            print('\\rEpisode {}\\tCurrent Score: {:.2f}\\tAverage Score: {:.2f}, episode took {:.2f} seconds'.format(i_episode,\n",
    "                np.max(scores_this_episode), np.mean(scores_window), episode_end_time - episode_start_time))\n",
    "        if i_episode % save_every == 0:\n",
    "            hundred_episodes_end_time = time.time()\n",
    "            torch.save(agents[0].actor_network_main.state_dict(), 'checkpoints/checkpoint_actor_{}.pth'.format(i_episode))\n",
    "            torch.save(agents[0].critic_network_main.state_dict(), 'checkpoints/checkpoint_critic_{}.pth'.format(i_episode))\n",
    "            hundred_episodes_start_time = time.time()\n",
    "        if (np.mean(scores_window) >= 0.5) & ~solved:\n",
    "            solved = True\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100,\\\n",
    "                np.mean(scores_window)))\n",
    "            torch.save(agents[0].actor_network_main.state_dict(), 'checkpoints/final_checkpoint_actor.pth')\n",
    "            torch.save(agents[0].critic_network_main.state_dict(), 'checkpoints/final_checkpoint_critic.pth')\n",
    "            break\n",
    "\n",
    "\n",
    "agents = []\n",
    "for _ in range(num_agents):\n",
    "    agents.append(Agent(num_instances=1,\n",
    "                  state_size=state_size,\n",
    "                  action_size=action_size,\n",
    "                  seed=0,\n",
    "                  actor_hidden_sizes_list=[64, 64],\n",
    "                  critic_hidden_sizes_list=[64, 64],\n",
    "                  gamma=0.99,\n",
    "                  num_iters_learn=1,\n",
    "                  actor_lr0=1e-3,\n",
    "                  critic_lr0=3e-3,\n",
    "                  weight_decay=0e-6,\n",
    "                  actor_reg_loss_weight=0.0025,\n",
    "                  update_every=1,\n",
    "                  noise_sigma=0.3,\n",
    "                  noise_sigma_decay=0.997,\n",
    "                  buffer_size=int(1e6),\n",
    "                  batch_size=128,\n",
    "                  use_prioritized_replay=True))\n",
    "agents[0].other_agent = agents[1]\n",
    "agents[1].other_agent = agents[0]\n",
    "            \n",
    "scores = []\n",
    "learn_maddpg(env, agents, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x137092860>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Tennis.app')\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "num_agents = states.shape[0]\n",
    "state_size = states.shape[1]\n",
    "\n",
    "actor_checkpoint = torch.load('checkpoints/final_checkpoint_actor.pth')\n",
    "agents = []\n",
    "for _ in range(num_agents):\n",
    "    agents.append(Agent(num_instances=1,\n",
    "                  state_size=state_size,\n",
    "                  action_size=action_size,\n",
    "                  seed=0,\n",
    "                  actor_hidden_sizes_list=[64, 64],\n",
    "                  critic_hidden_sizes_list=[64, 64],\n",
    "                  gamma=0.99,\n",
    "                  num_iters_learn=1,\n",
    "                  actor_lr0=0.0,\n",
    "                  critic_lr0=0.0,\n",
    "                  weight_decay=0e-6,\n",
    "                  actor_reg_loss_weight=0.0025,\n",
    "                  update_every=1,\n",
    "                  noise_sigma=0.0,\n",
    "                  noise_sigma_decay=0.0,\n",
    "                  buffer_size=int(1e6),\n",
    "                  batch_size=128,\n",
    "                  use_prioritized_replay=True))\n",
    "    agents[-1].actor_network_main.load_state_dict(actor_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env_info.vector_observations            # get the current state\n",
    "for t in range(1000):\n",
    "    actions = np.empty((num_agents, action_size))\n",
    "    for i, agent in enumerate(agents):\n",
    "        actions[i,:] = agent.act(states[i:(i+1),:])\n",
    "    env_info = env.step(actions)[env.brain_names[0]]        # send the actions to the environment\n",
    "    next_states = env_info.vector_observations   # get the next states\n",
    "    rewards = env_info.rewards                   # get the reward\n",
    "    dones = env_info.local_done                  # see if episode has finished\n",
    "    states = next_states       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
